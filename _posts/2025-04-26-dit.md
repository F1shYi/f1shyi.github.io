---
title: Diffusion Transformer for Music Generation
date: 2025-04-26 14:00:00 +/-0000
categories: [Paper, Digest]
tags: [paper digest, diffusion, music generation, transformer]     # TAG names should always be lowercase
author: fishyi
description: 本文整理了在音乐生成中使用DiT的相关工作。
math: true
---

## 前置知识

### ViT

原始论文：[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

证明了CNN并非建模图像的默认选择，transformer一样可以建模图像。

做法：
将图像（或者其feature map）分割成patch，有一个global linear projection将每个patch映射成一个token，然后将token序列输入transformer。

为了添加patch的位置（坐标）信息，对每个token添加位置编码。

为了添加类别信息，在token序列的开头添加learnable类别token，最后取类别token的隐表示作为图像的表示，再接分类器做图像分类。

如何处理不同分辨率的图像？保持patch的大小不变，只是对应了不同长度的序列，而transformer自然可以处理。至于位置编码，找到新图像的坐标对应原图像的坐标，然后进行2d插值计算。这个方法在fine-tuning的时候很有用，在实现中一般在low-res图像上预训练，在high-res图像上微调。注意微调时重新接了一个zero-init的分类器。

结论：在大规模数据集上预训练后，下游任务微调，表现不错，打平甚至小超SOTA CNN.

### Diffusion

DDPM formulation and CFG. 在CFG的框架下，问题归结为设计一个建模$$u(x_t, t, c)$$的网络结构。


## DiT

问题：建模$$u(x_t, t, c)$$?

输入：Noisy Latents $$x_t$$, diffusion timestep $$t$$, condition $$c$$.

如何将$$t$$和$$c$$的信息添加给$$x_t$$?

4种方式

1. 直接添加到token sequence里
2. Cross Attention
3. AdaLN
4. AdaLN-Zero

建模完成后回归均值和方差。


## Applications

### Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation

关注时间轴上的对齐（什么音频事件在什么位置发生）

做法：
1. 数据增强
   把不同的音频拼接，剪切在一起，然后相应地处理他们的text caption (temporal-aware data augmentation) e.g. xxx happens followed by yyy. zzz at all time. <x @ start>, <y @ end>, <z @ all>
2. Text encoder
   caption用CLAP （frozen）标，<event @ order> 用T5标（trainable），然后混在一起得到text embedding c.
3. 把音频变成梅尔频谱，然后再把梅尔频谱的频率轴压成20维，时间轴压成一半，得到(T, 20)的sequence，对这个sequence用transformer-based backbone预测噪声。（原论文中没有提c是如何注入到token sequence里的），源代码里好像有concat和cross-attn.两种选项


### LONG-FORM MUSIC GENERATION WITH LATENT DIFFUSION

1. prepended conditioning: diffusion timestep, timing condition
2. cross-attn.: text embedding, timing condition

### EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer

### MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence

### QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation

#### Related work

Make-an-audio 2和Stable audio 2都是只在time axis上划分,而本工作在time和frequency两个轴上划分

Make-an-audio 2，用gpt辅助的办法做数据增强

#### 模型结构创新

随机mask掉一些token,编码时只关注unmasked token，解码时把masked token换成learnable token.

#### 关注数据的质量

用frozen model给音乐打分，打分后

1. 在text caption前面添加low/medium/high quality的描述，然后将text embedding用cross attention添加信息到token sequence中
2. 在token sequence前面直接添加一个learnable quality embedding token.

#### 该工作中加条件的方法

1. 直接添加到token sequence里 （quality token）
2. Cross Attention （quaility-aware text embedding）
3. AdaLN （diffusion timestep embedding）
4. AdaLN-Zero

### Editing Music with Melody and Text: Using ControlNet for Diffusion Transformer

### TANGOFLUX: SUPER FAST AND FAITHFULTEXT TO AUDIO GENERATION WITH FLOW MATCHING AND CLAP-RANKED PREFERENCE OPTIMIZATION

### AudioX: Diffusion Transformer for Anything-to-Audio Generation


