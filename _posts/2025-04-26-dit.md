---
title: Diffusion Transformer for Music Generation
date: 2025-04-26 14:00:00 +/-0000
categories: [Paper, Digest]
tags: [paper digest, diffusion, music generation, transformer]     # TAG names should always be lowercase
author: fishyi
description: 本文整理了在音乐生成中使用DiT的相关工作。
math: true
---

## 前置知识

### ViT

原始论文：[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

证明了CNN并非建模图像的默认选择，transformer一样可以建模图像。

做法：
将图像（或者其feature map）分割成patch，有一个global linear projection将每个patch映射成一个token，然后将token序列输入transformer。

为了添加patch的位置（坐标）信息，对每个token添加位置编码。

为了添加类别信息，在token序列的开头添加learnable类别token，最后取类别token的隐表示作为图像的表示，再接分类器做图像分类。

如何处理不同分辨率的图像？保持patch的大小不变，只是对应了不同长度的序列，而transformer自然可以处理。至于位置编码，找到新图像的坐标对应原图像的坐标，然后进行2d插值计算。这个方法在fine-tuning的时候很有用，在实现中一般在low-res图像上预训练，在high-res图像上微调。注意微调时重新接了一个zero-init的分类器。

结论：在大规模数据集上预训练后，下游任务微调，表现不错，打平甚至小超SOTA CNN.

### Diffusion

DDPM formulation and CFG. 在CFG的框架下，问题归结为设计一个建模$$u(x_t, t, c)$$的网络结构。


## DiT

问题：建模$$u(x_t, t, c)$$?

输入：Noisy Latents $$x_t$$, diffusion timestep $$t$$, condition $$c$$.

如何将$$t$$和$$c$$的信息添加给$$x_t$$?

4种方式

1. 直接添加到token sequence里
2. Cross Attention
3. AdaLN
4. AdaLN-Zero

建模完成后回归均值和方差。


## Applications

对于一张图像，我们将其变换成一个patch sequence再建模，虽然把图像拉成一条看上去不太符合直觉，但是DiT证明了这样做是可行的。对于音频来说，其天然就是一个sequence，在图像上都已经可行了，在音频上这种更符合sequence本质的数据上，直觉上应该也是可行的。

那么DiT是如何用于音乐生成的工作中？

一般的LDM都包含以下三个组件：

1. VAE： 关注audio与latent $$z$$之间的相互转换.
2. Condition Encoder：关注在条件生成任务中如何编码条件信息得到condition embedding $$c$$（如利用预训练大语言模型编码text description得到text embedding）
3. LDM: 在隐空间中建模$$u(z_t, t, c)$$

将DiT应用到音乐生成工作中，一般需要关注在设计VAE中如何将audio编码成一个token sequence $$z$$，以及在LDM建模中是如何将condition embedding $$c, t$$的信息添加到noisy token sequence$$z_t$$中的。

### Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation

1. 任务：T2A
   额外关注sound event发生的顺序。
2. VAE
   将梅尔频谱的频率轴下采样4倍(80维到20维)，时间轴下采样两倍，得到一个`[20, T]`形状的sequence`z_t`
3. Condition Encoder:
   首先得到temporal-enhanced text caption，具体的做法通过拼接和混合已有的音频，并根据拼接和混合的方法生成一个标注了sound event发生的时许的text caption形如xxx happens followed by yyy. zzz at all time. <x @ start>, <y @ end>, <z @ all>。然后将原有的text caption用frozen CLAP进行编码，新得到的temporal-aware text caption用trainable T5编码，再将两个编码拼接得到最终的text embedding `c`
4. Transformer-based Denoising Backbone
   建模$$u(z_t, t, c)$$，原论文没有说明具体是如何添加条件信息`c`到`z_t`中的，但源代码中提供了concat和cross-attention两种选项。

### LONG-FORM MUSIC GENERATION WITH LATENT DIFFUSION

1. 任务：T2M
   关注长时间的音乐生成
2. VAE
   基于DAC
3. Condition Encoder：
   CLAP
4. DiT:
   prepended conditioning: diffusion timestep, timing condition
   cross-attn.: text embedding, timing condition

### EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer

1. 任务：T2A
   重点放在DiT架构的设计和创新上
2. VAE
   类似Stable Audio和DAC
3. Condition Encoder
   FLAN-T5
4. Efficient EzAudio-DiT
   - AdaLN-SOLA （用于添加diffusion timestep `t`的条件信息）
   - Long-skip Connection 保持细节（由于channel数太多）
  添加`c`的条件信息用的是cross-attn.
  训练：首先不带`c`，masked diffusion modelling，然后再加上`c`做cfg训练。

### QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation

1. 任务T2M
   关注音乐的质量
2. VAE
   在梅尔频谱上训了一个2D VAE，然后将频谱的latent做patchify
3. Condition Encoder
   用frozen model给音乐打分，得到分数quality，然后
   - 把quality以text的形式prepend到text caption里，然后用frozen LAION-CLAP编码得到quality-aware text embedding
   - 额外训一个learnable quality embedding
4. QA-MDT
   - Mask Strategy: 随机mask掉一些token，自编码时舍弃他们，自解码时换成learnable token
   - 加条件的方法：
     1. prepended token （quality token）
     2. Cross Attention （quaility-aware text embedding）
     3. AdaLN （diffusion timestep embedding）

### MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence

1. 任务：音乐和动作的同时生成
2. Bidirectional Contrastive Rhythmic VAE
   人体动作：$$[T,J,2]$$，表示$$T$$个时间帧，每个时间帧里$$J$$个关节的坐标。将其压缩为$$z_m$$，形状为$$[T_{zm},d]$$。
   音乐：将梅尔频谱压缩为$$z_{a}$$，形状为$$[T_{za},d]$$。
   总能确保$$T_{zm} = T_{za}$$且每个$$t$$上的$$d$$维特征向量$$z_m(t),z_a(t)$$总是temporal-aligned的，否则我们进行等间距的采样。
   一方面，$$z_m,z_a$$可以重建回人体动作和音乐。
   另一方面，如果$$t$$是拍点，那么用对比学习的方法要求$$z_m(t),z_a(t)$$相近。
   如何提取拍点？基于人体动作的幅度kinetic amplitude。是综合考虑了每个关节运动的距离和角度的一个函数。
3. DiT
   Motion to Music $$\epsilon_a$$
   在训练时将noisy $$z_a$$，condition $$z_m$$，diffusion timestep $$t$$全部concate到一起。
   Music to Motion $$\epsilon_m$$同理。
   Joint Generation: 首先分别利用两个模型$$\epsilon_a, \epsilon_m$$无条件地分别生成，生成到某个diffusion timestep后，用$$\epsilon_a$$直接去噪得到的clean $$z_a$$作为$$\epsilon_m$$的condition，用$$\epsilon_m$$直接去噪得到的clean $$z_m$$作为$$\epsilon_a$$的condition。以实现带引导的joint generation.

### A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation

1. 任务：音频和视频的同时生成
2. VAE:
   把视频，音频都用VAE编码成sequence，然后投影到同一个dimension
3. AVDiT
   架构创新：对一个sequence里的不同time-segment加不同的噪声等级（以前是对一整个sequence加同样的噪声，但对一个batch里的不同sequence加不同的噪声，现在是对同一个sequence里的不同time-segment加不同的噪声）
   加条件的方法：用AdaLN加diffusion timestep (a time vector here)的条件。
   对于X to Y, Y to X (X,Y来自不同模态)这样的条件，可以直接在推断的时候把condition modality的noise level置零，把剩下的noise level正常扩散。
   更一般地，还可以完成音视频的插值任务（把给定帧的noise level置零，把剩下的正常扩散），也可以joint generation.


### AudioX: Diffusion Transformer for Anything-to-Audio Generation

### Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models


### FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models

### Editing Music with Melody and Text: Using ControlNet for Diffusion Transformer

### TANGOFLUX: SUPER FAST AND FAITHFULTEXT TO AUDIO GENERATION WITH FLOW MATCHING AND CLAP-RANKED PREFERENCE OPTIMIZATION



