---
title: Diffusion Transformer for Music Generation
date: 2025-04-26 14:00:00 +/-0000
categories: [Paper, Digest]
tags: [paper digest, diffusion, music generation, transformer]     # TAG names should always be lowercase
author: fishyi
description: 本文整理了在音乐生成中使用DiT的相关工作。
math: true
---

## 前置知识

### ViT

原始论文：[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

证明了CNN并非建模图像的默认选择，transformer一样可以建模图像。

做法：
将图像（或者其feature map）分割成patch，有一个global linear projection将每个patch映射成一个token，然后将token序列输入transformer。

为了添加patch的位置（坐标）信息，对每个token添加位置编码。

为了添加类别信息，在token序列的开头添加learnable类别token，最后取类别token的隐表示作为图像的表示，再接分类器做图像分类。

如何处理不同分辨率的图像？保持patch的大小不变，只是对应了不同长度的序列，而transformer自然可以处理。至于位置编码，找到新图像的坐标对应原图像的坐标，然后进行2d插值计算。这个方法在fine-tuning的时候很有用，在实现中一般在low-res图像上预训练，在high-res图像上微调。注意微调时重新接了一个zero-init的分类器。

结论：在大规模数据集上预训练后，下游任务微调，表现不错，打平甚至小超SOTA CNN.

### Diffusion

DDPM formulation and CFG. 在CFG的框架下，问题归结为设计一个建模$$u(x_t, t, c)$$的网络结构。


## DiT

问题：建模$$u(x_t, t, c)$$?

输入：Noisy Latents $$x_t$$, diffusion timestep $$t$$, condition $$c$$.

如何将$$t$$和$$c$$的信息添加给$$x_t$$?

4种方式

1. 直接添加到token sequence里
2. Cross Attention
3. AdaLN
4. AdaLN-Zero

建模完成后回归均值和方差。


## Applications

对于一张图像，我们将其变换成一个patch sequence再建模，虽然把图像拉成一条看上去不太符合直觉，但是DiT证明了这样做是可行的。对于音频来说，其天然就是一个sequence，在图像上都已经可行了，在音频上这种更符合sequence本质的数据上，直觉上应该也是可行的。

那么DiT是如何用于音乐生成的工作中？

一般的LDM都包含以下三个组件：

1. VAE： 关注audio与latent $$z$$之间的相互转换.
2. Condition Encoder：关注在条件生成任务中如何编码条件信息得到condition embedding $$c$$（如利用预训练大语言模型编码text description得到text embedding）
3. LDM: 在隐空间中建模$$u(z_t, t, c)$$

将DiT应用到音乐生成工作中，一般需要关注在设计VAE中如何将audio编码成一个token sequence $$z$$，以及在LDM建模中是如何将condition embedding $$c, t$$的信息添加到noisy token sequence$$z_t$$中的。

### Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation

1. 任务：T2A
   额外关注sound event发生的顺序。
2. VAE
   将梅尔频谱的频率轴下采样4倍(80维到20维)，时间轴下采样两倍，得到一个`[20, T]`形状的sequence`z_t`
3. Condition Encoder:
   首先得到temporal-enhanced text caption，具体的做法通过拼接和混合已有的音频，并根据拼接和混合的方法生成一个标注了sound event发生的时许的text caption形如xxx happens followed by yyy. zzz at all time. <x @ start>, <y @ end>, <z @ all>。然后将原有的text caption用frozen CLAP进行编码，新得到的temporal-aware text caption用trainable T5编码，再将两个编码拼接得到最终的text embedding `c`
4. Transformer-based Denoising Backbone
   建模$$u(z_t, t, c)$$，原论文没有说明具体是如何添加条件信息`c`到`z_t`中的，但源代码中提供了concat和cross-attention两种选项。

### LONG-FORM MUSIC GENERATION WITH LATENT DIFFUSION

1. 任务：T2M
   关注长时间的音乐生成
2. VAE
   基于DAC
3. Condition Encoder：
   CLAP
4. DiT:
   prepended conditioning: diffusion timestep, timing condition
   cross-attn.: text embedding, timing condition

### EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer

1. 任务：T2A
   重点放在DiT架构的设计和创新上
2. VAE
   类似Stable Audio和DAC
3. Condition Encoder
   FLAN-T5
4. Efficient EzAudio-DiT
   - AdaLN-SOLA （用于添加diffusion timestep `t`的条件信息）
   - Long-skip Connection 保持细节（由于channel数太多）
  添加`c`的条件信息用的是cross-attn.
  训练：首先不带`c`，masked diffusion modelling，然后再加上`c`做cfg训练。

### QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation

1. 任务T2M
   关注音乐的质量
2. VAE
   在梅尔频谱上训了一个2D VAE，然后将频谱的latent做patchify
3. Condition Encoder
   用frozen model给音乐打分，得到分数quality，然后
   - 把quality以text的形式prepend到text caption里，然后用frozen LAION-CLAP编码得到quality-aware text embedding
   - 额外训一个learnable quality embedding
4. QA-MDT
   - Mask Strategy: 随机mask掉一些token，自编码时舍弃他们，自解码时换成learnable token
   - 加条件的方法：
     1. prepended token （quality token）
     2. Cross Attention （quaility-aware text embedding）
     3. AdaLN （diffusion timestep embedding）


### MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence


### Editing Music with Melody and Text: Using ControlNet for Diffusion Transformer

### TANGOFLUX: SUPER FAST AND FAITHFULTEXT TO AUDIO GENERATION WITH FLOW MATCHING AND CLAP-RANKED PREFERENCE OPTIMIZATION

### AudioX: Diffusion Transformer for Anything-to-Audio Generation


